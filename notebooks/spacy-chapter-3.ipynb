{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/python/3.10.4/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (66.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "2023-01-23 04:19:56.014005: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-23 04:19:58.780651: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-23 04:19:58.780682: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-23 04:20:04.161259: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 04:20:04.161432: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-23 04:20:04.161449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-23 04:20:08.918028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-23 04:20:08.930865: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-23 04:20:08.930922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-db6001): /proc/driver/nvidia/version does not exist\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.11)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (66.0.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) What happens when you call nlp?\n",
    "\n",
    "```\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "```\n",
    "\n",
    "~~Run the tagger, parser and entity recognizer and then the tokenizer.~~\n",
    "\n",
    "**Tokenize the text and apply each pipeline component in order.`**\n",
    "\n",
    "~~Connect to the spaCy server to compute the result and return it.~~\n",
    "\n",
    "~~Initialize the language, add the pipeline and load in the binary model weights.~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's correct! \n",
    "The tokenizer turns a string of text into a Doc object. spaCy then applies every component in the pipeline on document, in order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Inspecting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 21:32:31.648581: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 21:32:31.787267: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-22 21:32:31.787287: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-22 21:32:32.496094: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 21:32:32.496175: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 21:32:32.496184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-22 21:32:33.521887: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-22 21:32:33.521922: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-22 21:32:33.521941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-db6001): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f495af93e20>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f495af93280>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f495b0b8f90>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f495ad6b2c0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f495adaccc0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f495b0b90e0>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Custom pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Use cases for custom components\n",
    "\n",
    "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
    "\n",
    "1. Updating the trained pipelines and improving their predictions\n",
    "\n",
    "2. Computing your own values based on tokens and their attributes\n",
    "\n",
    "3. Adding named entities, for example based on a dictionary\n",
    "\n",
    "4. Implementing support for an additional language\n",
    "\n",
    "~~1 and 2.~~\n",
    "\n",
    "~~1 and 3.~~\n",
    "\n",
    "~~1 and 4.~~\n",
    "\n",
    "```python\n",
    "2 and 3.\n",
    "```\n",
    "\n",
    "~~2 and 4.~~\n",
    "\n",
    "~~3 and 4.~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's correct! Custom components are great for adding custom values to documents, tokens and spans, and customizing the doc.ents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Simple components\n",
    "\n",
    "The example shows a custom component that prints the number of tokens in a document. Can you complete it?\n",
    "\n",
    "* Complete the component function with the doc’s length.\n",
    "* Add the \"length_component\" to the existing pipeline as the first component.\n",
    "* Try out the new pipeline and process any text with the nlp object – for example “This is a sentence.”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 22:18:10.234642: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 22:18:13.026137: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-22 22:18:13.026167: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-22 22:18:18.465237: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 22:18:18.465458: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 22:18:18.465469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-22 22:18:23.161242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-22 22:18:23.173921: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-22 22:18:23.173963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-db6001): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This document is 5 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Complex components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 23:37:22.409218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 23:37:24.971045: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-22 23:37:24.971077: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-22 23:37:30.057220: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 23:37:30.057420: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 23:37:30.057437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-22 23:37:34.457777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-22 23:37:34.469697: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-22 23:37:34.469741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-db6001): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 6303828839600189595), ('Golden Retriever', 6303828839600189595)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label) for ent in doc.ents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remember that the matches are a list of (match_id, start, end) tuples.\n",
    "* The Span class takes 4 arguments: the parent doc, the start index, the end index and the label.\n",
    "* To add a component after another, use the after keyword argument on nlp.add_pipe. Remember to use the string name of the component to add it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (9) Setting extension attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"Vy\\x1d\\x96W\\xf3w\\xaa\\x15\\xe4\\xa0\\xfd.\\x9a\\xb4\\xb2Z\\x9c _\\x8a,3\\x98%\\xb4\\x16L\\xe1\\x0b\\xe7\\x94\\x83vU5\\x9cg%\\t\\xfc>N\\x07'\\x1a&\\x8c}\\x16\\x1d\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127\", b'.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00']\n",
      "Bad pipe message: %s [b'\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01']\n",
      "Bad pipe message: %s [b\"):\\xaaG\\xf7Vb\\xc8\\xdcna\\x17p\\x1f\\xfe\\xe8h]\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\"]\n",
      "Bad pipe message: %s [b'R\\x92\\xa8\\xbd\\xc5\\xa7\\xf4\\xec\\xed\\x1de\\xc1\\x8f\\xa5lSB7\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8']\n",
      "Bad pipe message: %s [b\"\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\"]\n",
      "Bad pipe message: %s [b'\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08']\n",
      "Bad pipe message: %s [b'\\x08\\n\\x08\\x0b\\x08\\x04\\x08', b'\\x06\\x04\\x01\\x05']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'5\\x91\\xf7\\x9f\\x9d!UA@y\\xebHE\\n\\xe4\\x8a\\xd31\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17']\n",
      "Bad pipe message: %s [b'\\xa3_\\xa54\\x8dor\\xefJ\\xad\\x87\\xeb/\\x84\\xff\\xf6\\xb8\\xca\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0', b':\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00']\n",
      "Bad pipe message: %s [b\"y-\\ta\\xe4\\xfe|\\xbe\\xb6\\x0f0*\\x07\\xe2\\x98\\\\!\\t\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\"]\n",
      "Bad pipe message: %s [b'\\x06\\x02\\x06\\x03\\x05', b'', b'\\x03', b'\\x04\\x02\\x04', b'\\x01\\x03', b'\\x03', b'\\x02', b'\\x03']\n",
      "Bad pipe message: %s [b'\\xd9\\x00\\xbd\\x91\\xe6\\x18a\\xf0\\xdb\\x95\\xe4|\\x83\\xf6\\x91\\xa4\\x1a\\x98\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00']\n",
      "Bad pipe message: %s [b\"\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\"]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False, force=True)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2\n",
    "* Use Token.set_extension to register \"reversed\" (getter function get_reversed).\n",
    "* Print its value for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that extension attributes are available via the ._ property. For example, doc._.has_color."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) Setting extension attributes 2\n",
    "\n",
    "Part 1\n",
    "\n",
    "* Complete the get_has_number function .\n",
    "* Use Doc.set_extension to register \"has_number\" (getter get_has_number) and print its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "* Use Span.set_extension to register \"to_html\" (method to_html).\n",
    "* Call it on doc[0:2] with the tag \"strong\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to_html() missing 1 required positional argument: 'tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m doc \u001b[39m=\u001b[39m nlp(\u001b[39m\"\u001b[39m\u001b[39mHello world, this is a sentence.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m span \u001b[39m=\u001b[39m doc[\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m \u001b[39mprint\u001b[39m(span\u001b[39m.\u001b[39;49m_\u001b[39m.\u001b[39;49mto_html(\u001b[39m\"\u001b[39m\u001b[39mstrong\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/spacy/tokens/underscore.py:50\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     48\u001b[0m default, method, getter, setter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extensions[name]\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m getter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m getter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_obj)\n\u001b[1;32m     51\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     method_partial \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(method, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj)\n",
      "\u001b[0;31mTypeError\u001b[0m: to_html() missing 1 required positional argument: 'tag'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", getter=to_html, force=True)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Method extensions can take one or more arguments. For example: doc._.some_method(\"argument\").\n",
    "* The first argument passed to the method is always the Doc, Token or Span object the method was called on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
